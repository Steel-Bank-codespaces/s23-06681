{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## Introduction to data files\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Today we are going to jump right into loading data from files into numpy arrays. Once we have the arrays, then we will then start learning how to use them.\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Loading data from a text file\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Usually, data will be in a file, and we need to read it in. There are *many* data formats to get familiar with, and *many* ways to read them in. Let's start by looking at data from a Raman spectrometer.\n", "\n", "In this directory there is a file called 01-raman.txt. The file name suggests it is Raman data, but only because of its name. The extension *suggests* this is text data, but does not tell us the format.\n", "\n", "*You cannot in general rely on the extension to tell you what format a file is in. That is an advertisement, and only a convention that is used. You can rename a file and its extension at any time and not change the data.*\n", "\n", "It is not obvious what is in the file. Unless you made the file, or someone has told you what is in it, the only thing you can do is inspect the file manually. Let's look at the number of lines and the first 10 lines here.\n", "\n", "A conventional way to do this is in a shell with the `head` command. In Jupyter you can prefix a line with `!` to indicate it is a shell command\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["! head raman.txt\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Alternatively, you can use a magic cell. Here instead of the first few lines, we look at the last few lines with the `tail` command.\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%bash\n", "tail raman.txt\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can also do that in Python by opening the file, reading the lines and then closing the file. We print the number of lines and the first 10 here.\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["f = open('raman.txt')\n", "lines = f.readlines()\n", "f.close()\n", "\n", "print(f'{len(lines)} lines in the file')\n", "print(lines[0:10])\n", "print(''.join(lines[0:10]))\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You might notice the \"\\t\" and \"\\n\" characters. The first is a tab, and the second is a new line (the new line is what defines a line, each line ends with this character). This looks like a *tab-delimited* text file, with two columns where the numbers are separated by tabs. Unfortunately, there is no information about *what* the columns are. You have to know from some other context. This file is from a Raman spectrometer, and the first column is the wavenumber, and the second column is the intensity.\n", "\n", "It is important that you remember to close the file. If you are reading lots of files, you can run into operating system errors if you open too many open files. Python is at your service with a *context manager*. This approach will *automatically* close the file when it goes of scope.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with open('raman.txt') as f:\n", "    # f is only open and available inside this body\n", "    lines = f.readlines()\n", "    print(f'Inside: {f.closed}')\n", "\n", "print(f'Outside: {f.closed}')\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Simple loading into numpy\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["There is an easy way for us to read this file so we can get the data into Python for analysis, via `np.loadtxt`. Let's start with the simplest method. We simply use the filename as the only argument.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "data = np.loadtxt('raman.txt')\n", "data\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can see the shape here, which confirms that we have read the number of lines, and that there are two columns.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data.shape\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It is often helpful to separate this array into variables. The first column is the wavenumber and the second column is the intensity. We use indexing to extract these into variables. This is a 2D array, and we index using a notation of `[row, col]` where `row` indicates which row(s) we want, and `col` indicates which column(s) we want. If you use a `:` it means all of them. Indexing can be very complex ([https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html)), and we will return to it many times as we do more and more sophisticated things.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["wavenumber = data[:, 0]\n", "intensity = data[:, 1]\n", "\n", "(wavenumber.shape, intensity.shape)\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, these variables are 1D arrays.\n", "\n", "Alternatively, you can *unpack* an array. When you iterate over an array, you iterate over its rows. If we transpose this data, we will change it from two columns with a lot of rows into two rows with a lot of columns.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["wavenumber, intensity = data.T\n", "(wavenumber.shape, intensity.shape)\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Here are some examples of indexing these arrays:\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# first and last element\n", "(wavenumber[0], wavenumber[-1])\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# first five and last 5 elements\n", "(intensity[0:5], intensity[-5:])\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You might be tempted to get the last five elements like this, but it is wrong. Remember in indexing the last element in the slice is not included. Since you specify -1 in the slice it means up to *but not including* the last element. In contrast, `[-5:]` means from the fifth to last element to the end, *including the end*.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["intensity[-5:-1]\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["There are even fancier ways to slice this, e.g. to get every other point or every nth point, or even a set of selected points. We will see these later.\n", "\n", "That is a lot of data. How do we see what is in it? Visualization.\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Simple visualization of the data\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["[matplotlib](https://matplotlib.org/) is the primary plotting tool we will use. There are a few others that are popular, especially [seaborn](https://seaborn.pydata.org/). It is good advice that when you want to make a plot, a good way to do it is find an example plot like it and see if you can adapt that code to your data.\n", "\n", "-   **matplotlib sample plots:** [https://matplotlib.org/tutorials/introductory/sample_plots.html](https://matplotlib.org/tutorials/introductory/sample_plots.html)\n", "-   **matplotlib gallery:** [https://matplotlib.org/gallery/index.html](https://matplotlib.org/gallery/index.html)\n", "-   **seaborn gallery:** [https://seaborn.pydata.org/examples/index.html](https://seaborn.pydata.org/examples/index.html)\n", "\n", "There are many other plotting libraries, e.g. [plotly](https://plot.ly/) and [altair](https://altair-viz.github.io/gallery/index.html#gallery-category-interactive-charts) that are more suitable for web graphics. These have much more complex APIs and it takes a long time to get familiar with them.\n", "\n", "We will start simple. Let's start by telling the notebook how to use matplotlib and then load it. The line starting with a % is called a [magic command](https://ipython.readthedocs.io/en/stable/interactive/magics.html), and it is only valid in Jupyter-Python, or a jupyter notebook. It is *not* valid Python. It tells the Jupyter kernel how to handle plots, and in this case we are telling it to put the plot inline as a PNG image in the notebook.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can make a plot as simple as this:\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(wavenumber, intensity)\n", "plt.xlabel('Wavenumber')\n", "plt.ylabel('Intensity');\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, we have a graphical representation of this data set. You can interact with it in the notebook. You can see the coordinates, and zoom in on regions. Suppose we want to make sure we *reproducibly* zoom in to a region though. That requires us to script it so that you have a written record of what you did.\n", "\n", "Let's make a plot of a subset of the spectrum, say the points where the wavenumber is at least 1000, and less than 1500. To do that, we need a way to select the data subset. We can use indexing for this purpose. We define a *boolean* array where it is True for the points we want, and False for the ones we do not want. The comparison operators will do this, and the logical & operator combines the two conditions. Note these are all elementwise operations.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# indices of points where 1000 >= wavenumber > 1500\n", "# True means it meets the criteria, False means it does not\n", "ind = (wavenumber >= 1000) & (wavenumber < 1500)\n", "ind\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["These two arrays have the same shape, that is we evaluated the condition on every single point.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ind.shape, wavenumber.shape\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["How many points did we get? We can find out by counting the number of True values. In Python, True=1, and False=0.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["True + True\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["So, we get the number of True values just by summing them here.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np.sum(ind)\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can also plot the values like this:\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(wavenumber, ind)\n", "plt.xlabel('wavenumber')\n", "plt.ylabel('ind');\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["These are all ways to check that we have the indices we want. Now we use that array to select the *portion* of the data we want. When we use the Boolean array to index with, the result is a shorter array containing only values where the Boolean array was True.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["wavenumber[ind].shape\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We have to index *both* the x and y data to make this plot, because we can only plot arrays with the matching dimensions.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure()\n", "plt.plot(wavenumber[ind], intensity[ind])\n", "plt.xlabel('Wavenumber')\n", "plt.ylabel('Intensity');\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Summary\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["There was a lot of explanation above, so the brevity of this code may have been lost. Here is what we need to read the data and plot it (assuming you have already done the imports).\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data = np.loadtxt('raman.txt')\n", "\n", "wavenumber, intensity = data.T  # the transpose has data in rows for unpacking\n", "ind = (wavenumber >= 1000) & (wavenumber < 1500)\n", "\n", "plt.figure()\n", "plt.plot(wavenumber[ind], intensity[ind])\n", "plt.xlabel('Wavenumber')\n", "plt.ylabel('Intensity');\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this simple example we have learned how to:\n", "\n", "1.  Retrieve data from a url into a local file\n", "2.  Read a local file of tab-delimited data into numpy arrays\n", "3.  Visualize the arrays\n", "4.  Select a subset of the arrays and visualize the subset.\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Data formats\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The data in the previous example was loaded from a tab-delimited text file. There are many other kinds of files you might have to read from.\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Simple delimited text files\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In a delimited text file, the data is represented in each line, with the values separated by a *delimiter*. In the tab-delimited data, tab characters are the separators. The other common delimiter is a comma, and these files are often called comma-separated value (CSV) files. For now, we limit the discussion to files containing numbers that are separated by commas.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with open('raman.csv') as f:\n", "    lines = f.readlines()\n", "\n", "print(len(lines))\n", "print(lines[0:10])\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can see the numbers in each line are now separated by commas. `np.loadtxt` works for this file too, but we have to specify that the delimiter is a comma. See the [documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html) on `np.loadtxt` for more options.\n", "\n", "Note: CSV is surprisingly complex, and there is a [dedicated library](https://docs.python.org/3/library/csv.html) for parsing it. We will stick to simple numerical csv files, which are easy to load in numpy.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np.loadtxt('raman.csv', delimiter=',')\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's do one more thing that is convenient here, which is we directly *unpack* the columns into variable names:\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["wave, intensity = np.loadtxt('raman.csv', delimiter=',', unpack=True)\n", "plt.plot(wave, intensity)\n", "plt.xlabel('Wavenumber')\n", "plt.ylabel('intensity');\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### more complex delimited files\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This data is located at [https://www.itl.nist.gov/div898/handbook/datasets/MODEL-4_4_4.DAT](https://www.itl.nist.gov/div898/handbook/datasets/MODEL-4_4_4.DAT). There is a copy of the file in this directory.\n", "\n", "If we examine the first few lines of this file, it appears that the first two lines are considered headers that tell you what is in the file. We need to skip these when reading the data. Also, each line appears as a string, with the values lining up in columns. This is sometimes called a fixed delimiter file.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with open('p-t.dat') as f:\n", "    print(''.join(f.readlines()[0:5]))\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["I did something kind of fancy there. First, we read the lines and slice out the first 5 of them. That gives us a list of 5 lines. Then, I rejoin them so we get a single string, and then I printed it. That is to avoid getting two newlines (one from the line, and one from the print). We can tell print not to add a new line like this.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with open('p-t.dat') as f:\n", "    for line in f.readlines()[0:5]:\n", "        print(line, end='')\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Either way, you can see there are two non-data lines here. We can still load this file with `np.loadtxt` we just tell it to skip the first two lines.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["d = np.loadtxt('p-t.dat', skiprows=2)\n", "d\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You may note, however, that the first two columns are floats, and not integers. We can fix that (if it matters), at the cost of some complexity. We specify the format of each column in the `dtype` argument. This leads to a structured data array ([https://docs.scipy.org/doc/numpy/user/basics.rec.html](https://docs.scipy.org/doc/numpy/user/basics.rec.html)).\n", "\n", "The dtype formats are documented at [https://docs.scipy.org/doc/numpy/reference/arrays.dtypes.html#arrays-dtypes-constructing](https://docs.scipy.org/doc/numpy/reference/arrays.dtypes.html#arrays-dtypes-constructing). The ones we use here are for 32-bit integers and floats.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["d = np.loadtxt('p-t.dat', skiprows=2,\n", "               dtype={'names': ('run order', 'day', 'ambient temperature', 'temperature', 'pressure', 'fitted value', 'residual'),\n", "                      'formats': ('i4', 'i4', 'f4', 'f4', 'f4', 'f4', 'f4')})\n", "d\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This array should look different to you. First, each row is in (), and the dtype at the end looks different, with names and formats.\n", "\n", "One benefit of this is you can now use human readable names to select the various columns. We will return to this idea later when we learn about Pandas.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(d['temperature'], d['pressure'], 'b. ')\n", "plt.xlabel('Temperatue')\n", "plt.ylabel('pressure');\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note that this is not a 2D array, even though it looks like one! It is considered a 1D array of records.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["d.shape\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It is not possible to slice this array like we did before. Instead we select columns by name.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["d['temperature']\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Summary\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This data file was a text file, with a *header* that described some information about the data. The data itself was still delimited, but each column had a different type of data, e.g. in this case integers and floats. Some files may also have string data in columns. As the data gets more heterogeneous, it gets more challenging to read it into variables. In the worst case scenario, you can write a custom data parser, but this is hard work that should usually be avoided if possible.\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### json data\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["json (javascript object notation) is another convenient data format ([https://www.json.org/json-en.html](https://www.json.org/json-en.html)). This data format stores data in a \"key=value\" format, and when you load it, you get a dictionary of the data. Let's see the first few characters of this file. Why the first few characters? A json file may be only one line as defined by newlines.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with open('raman.json') as f:\n", "    print(len(f.readlines()))\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["But you can see there are a lot of characters:\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with open('raman.json') as f:\n", "    print(len(f.read()))\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's see the first 50 of the characters.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with open('raman.json') as f:\n", "    print(''.join(f.read()[0:50]))\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Here, \"wavenumber\" is a key, and it has a value (the thing on the other side of the :) that is an array of numbers. Luckily, it is easy to load this in Python with the json library.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import json\n", "\n", "with open('01-raman.json') as f:\n", "    d = json.load(f)\n", "\n", "type(d), d.keys()\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note that this does not read the data into numpy arrays. We have a list for these instead. We can convert them to arrays if we need to.\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["type(['wavenumber'])\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(d['wavenumber'], d['intensity'])\n", "plt.xlabel('Wavenumber')\n", "plt.ylabel('Intensity');\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It is fine to plot with these lists, but if you need to do any array operations or analysis on them, you may need to convert them to arrays first (although many numpy functions work on lists just fine).\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### There are many more data formats\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["There are many more formats. For most of these, there are standard libraries for reading them data into an array form that you can use. You should usually search for a library that can read your data first, rather than try to implement your own parser.\n", "\n", "-   **netcdf:** [https://unidata.github.io/netcdf4-python/netCDF4/index.html](https://unidata.github.io/netcdf4-python/netCDF4/index.html)\n", "-   **hdf5:** [https://www.h5py.org/](https://www.h5py.org/)\n", "-   **xml:** [https://docs.python.org/3/library/xml.html](https://docs.python.org/3/library/xml.html)\n", "-   **matlab .mat files:** [https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.loadmat.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.loadmat.html)\n", "-   **binary data files:** [https://www.devdungeon.com/content/working-binary-data-python](https://www.devdungeon.com/content/working-binary-data-python)\n", "-   **scipy io routines:** [https://docs.scipy.org/doc/scipy/reference/io.html](https://docs.scipy.org/doc/scipy/reference/io.html)\n", "-   **molecular simulation files:** [https://wiki.fysik.dtu.dk/ase/ase/io/io.html#module-ase.io](https://wiki.fysik.dtu.dk/ase/ase/io/io.html#module-ase.io)\n", "-   **yaml:** [https://pyyaml.org/wiki/PyYAMLDocumentation](https://pyyaml.org/wiki/PyYAMLDocumentation)\n", "-   **config files:** [https://docs.python.org/3/library/configparser.html](https://docs.python.org/3/library/configparser.html)\n", "-   **Excel:** [https://openpyxl.readthedocs.io/en/stable/index.html](https://openpyxl.readthedocs.io/en/stable/index.html)\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Summary\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Today the focus was on orienting ourselves to the main ideas of reading data files. These are:\n", "\n", "1.  What kind of data file is it?\n", "2.  Is there a library function that makes it easy to read?\n", "3.  How do you get the data into arrays and variables for further analysis.\n", "\n", "We also examined how to make simple visualizations of the data.\n", "\n", "\n", "\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.7"}, "org": null, "widgets": {"application/vnd.jupyter.widget-state+json": {"state": {}, "version_major": 2, "version_minor": 0}}}, "nbformat": 4, "nbformat_minor": 4}